{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70089,"databundleVersionId":9515283,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"papermill":{"default_parameters":{},"duration":4336.484731,"end_time":"2024-09-10T16:05:58.969021","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-09-10T14:53:42.484290","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport sys\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.509079,"end_time":"2024-09-10T14:53:46.349614","exception":false,"start_time":"2024-09-10T14:53:45.840535","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-11T14:13:27.931968Z","iopub.execute_input":"2024-09-11T14:13:27.932549Z","iopub.status.idle":"2024-09-11T14:13:27.945635Z","shell.execute_reply.started":"2024-09-11T14:13:27.932501Z","shell.execute_reply":"2024-09-11T14:13:27.944134Z"},"trusted":true},"execution_count":267,"outputs":[{"name":"stdout","text":"/kaggle/input/um-game-playing-strength-of-mcts-variants/sample_submission.csv\n/kaggle/input/um-game-playing-strength-of-mcts-variants/train.csv\n/kaggle/input/um-game-playing-strength-of-mcts-variants/test.csv\n/kaggle/input/um-game-playing-strength-of-mcts-variants/concepts.csv\n/kaggle/input/um-game-playing-strength-of-mcts-variants/kaggle_evaluation/mcts_gateway.py\n/kaggle/input/um-game-playing-strength-of-mcts-variants/kaggle_evaluation/__init__.py\n/kaggle/input/um-game-playing-strength-of-mcts-variants/kaggle_evaluation/mcts_inference_server.py\n/kaggle/input/um-game-playing-strength-of-mcts-variants/kaggle_evaluation/core/templates.py\n/kaggle/input/um-game-playing-strength-of-mcts-variants/kaggle_evaluation/core/base_gateway.py\n/kaggle/input/um-game-playing-strength-of-mcts-variants/kaggle_evaluation/core/relay.py\n/kaggle/input/um-game-playing-strength-of-mcts-variants/kaggle_evaluation/core/kaggle_evaluation.proto\n/kaggle/input/um-game-playing-strength-of-mcts-variants/kaggle_evaluation/core/__init__.py\n/kaggle/input/um-game-playing-strength-of-mcts-variants/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n/kaggle/input/um-game-playing-strength-of-mcts-variants/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n/kaggle/input/um-game-playing-strength-of-mcts-variants/kaggle_evaluation/core/generated/__init__.py\n","output_type":"stream"}]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df","metadata":{"papermill":{"duration":0.03421,"end_time":"2024-09-10T14:53:46.393282","exception":false,"start_time":"2024-09-10T14:53:46.359072","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = import_data('/kaggle/input/um-game-playing-strength-of-mcts-variants/train.csv')\ntest = import_data('/kaggle/input/um-game-playing-strength-of-mcts-variants/test.csv')\n","metadata":{"papermill":{"duration":43.711495,"end_time":"2024-09-10T14:54:30.113811","exception":false,"start_time":"2024-09-10T14:53:46.402316","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-11T14:13:27.948262Z","iopub.execute_input":"2024-09-11T14:13:27.949236Z","iopub.status.idle":"2024-09-11T14:13:38.614238Z","shell.execute_reply.started":"2024-09-11T14:13:27.949182Z","shell.execute_reply":"2024-09-11T14:13:38.612982Z"},"trusted":true},"execution_count":268,"outputs":[]},{"cell_type":"code","source":"\n  # Get a summary of the dataset including column types and non-null counts\n","metadata":{"papermill":{"duration":0.060177,"end_time":"2024-09-10T14:54:30.183865","exception":false,"start_time":"2024-09-10T14:54:30.123688","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-11T14:13:38.615888Z","iopub.execute_input":"2024-09-11T14:13:38.616261Z","iopub.status.idle":"2024-09-11T14:13:38.621856Z","shell.execute_reply.started":"2024-09-11T14:13:38.616222Z","shell.execute_reply":"2024-09-11T14:13:38.620521Z"},"trusted":true},"execution_count":269,"outputs":[]},{"cell_type":"code","source":"y = train[['utility_agent1']]\nX = train.drop('utility_agent1')\nX = X.drop(['num_draws_agent1', 'num_losses_agent1', 'num_wins_agent1'])","metadata":{"papermill":{"duration":0.659311,"end_time":"2024-09-10T14:54:32.025203","exception":false,"start_time":"2024-09-10T14:54:31.365892","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-11T14:13:38.623668Z","iopub.execute_input":"2024-09-11T14:13:38.624096Z","iopub.status.idle":"2024-09-11T14:13:38.637560Z","shell.execute_reply.started":"2024-09-11T14:13:38.624055Z","shell.execute_reply":"2024-09-11T14:13:38.636214Z"},"trusted":true},"execution_count":270,"outputs":[]},{"cell_type":"code","source":"def drop_missing_data(df):\n# Now you can safely call isnull()\n    missing_percentage = X.isna().mean() * 100\n# Drop columns with more than 50% missing values\n    df = df.drop(columns=missing_percentage[missing_percentage > 50].index)\n    return df\nX = drop_missing_data(X)","metadata":{"papermill":{"duration":1.159711,"end_time":"2024-09-10T14:54:31.355463","exception":false,"start_time":"2024-09-10T14:54:30.195752","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-11T14:13:38.640136Z","iopub.execute_input":"2024-09-11T14:13:38.640593Z","iopub.status.idle":"2024-09-11T14:13:38.695007Z","shell.execute_reply.started":"2024-09-11T14:13:38.640551Z","shell.execute_reply":"2024-09-11T14:13:38.693177Z"},"trusted":true},"execution_count":271,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[271], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mmissing_percentage[missing_percentage \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m50\u001b[39m]\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m---> 10\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdrop_missing_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[271], line 2\u001b[0m, in \u001b[0;36mdrop_missing_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop_missing_data\u001b[39m(df):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mX\u001b[49m, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m      3\u001b[0m         X \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Now you can safely call isnull()\u001b[39;00m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'X' referenced before assignment"],"ename":"UnboundLocalError","evalue":"local variable 'X' referenced before assignment","output_type":"error"}]},{"cell_type":"code","source":"def drop_nonunique_values(df):\n    global unique_counts\n    unique_counts = X.nunique()\n    print(unique_counts)\n# Drop columns with only one unique value\n    df = df.drop(columns=unique_counts[unique_counts == 1].index)\n    return df\nX = drop_nonunique_values(X)","metadata":{"papermill":{"duration":1.904333,"end_time":"2024-09-10T14:54:33.939959","exception":false,"start_time":"2024-09-10T14:54:32.035626","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-11T14:13:38.696838Z","iopub.status.idle":"2024-09-11T14:13:38.697299Z","shell.execute_reply.started":"2024-09-11T14:13:38.697087Z","shell.execute_reply":"2024-09-11T14:13:38.697111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def columns_transform(X):\n# Split 'agent1' into 5 parts and assign to new columns\n    global ct\n    X[['agent1_part1', 'agent1_part2', 'agent1_part3', 'agent1_part4', 'agent1_part5']] = X['agent1'].str.split('-', expand=True)\n\n# Split 'agent2' into 5 parts and assign to new columns\n    X[['agent2_part1', 'agent2_part2', 'agent2_part3', 'agent2_part4', 'agent2_part5']] = X['agent2'].str.split('-', expand=True)\n\n\n    X = X.drop(['Id','agent1','agent2'], axis=1)\n\n    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n\n    num_feat = X.select_dtypes('number').columns\n    feature_names = X.columns\n\n    from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n    cat_encoder = OrdinalEncoder()\n    std = StandardScaler()\n    from sklearn.compose import ColumnTransformer\n    ct = ColumnTransformer([('cat', cat_encoder, categorical_features), ('std', StandardScaler(), num_feat)])\n    X = ct.fit_transform(X)\n    return X\nX = columns_transform(X)\n\n\n","metadata":{"papermill":{"duration":1.085271,"end_time":"2024-09-10T14:54:35.036105","exception":false,"start_time":"2024-09-10T14:54:33.950834","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-11T14:13:38.698917Z","iopub.status.idle":"2024-09-11T14:13:38.699322Z","shell.execute_reply.started":"2024-09-11T14:13:38.699128Z","shell.execute_reply":"2024-09-11T14:13:38.699148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model():\n    from sklearn.decomposition import PCA\n    global pca, lgbm_reg\n# Reduce to 100 principal components\n    pca = PCA(n_components=100)\n    X_pca = pca.fit_transform(X)\n    X_train, X_valid, y_train, y_valid = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n    import lightgbm as lgb\n    lgbm_reg = lgb.LGBMRegressor(\n        learning_rate=0.05,\n        num_leaves=50,\n        n_estimators=1500,\n        min_child_samples=42,\n        subsample=0.8,\n        colsample_bytree=0.9,\n        reg_alpha=0.001,\n        reg_lambda=0.02,\n        min_split_gain=0.001,\n        min_child_weight=0.002)        # Use GPU for faster training\n    \n    lgbm_reg.fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n    y_predict = lgbm_reg.predict(X_valid)\n\n# Evaluate the model\n    from sklearn.metrics import mean_squared_error\n    import numpy as np\n\n# Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(y_valid, y_predict))\n    print(f'Validation RMSE: {rmse:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-11T14:13:38.701743Z","iopub.status.idle":"2024-09-11T14:13:38.702186Z","shell.execute_reply.started":"2024-09-11T14:13:38.701979Z","shell.execute_reply":"2024-09-11T14:13:38.702001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import polars as pl  # Assuming you're using the `pl.Series` from the Polars library\n\ncounter = 0\ndef predict(test):\n    global counter\n    \n    if counter == 0:\n        # Train the model only the first time predict is called\n        train_model()  # Ensure train_model() trains the LightGBM model (lgbm_reg)\n    \n    counter += 1\n    \n    # Step 1: Preprocess the test data\n    test = drop_missing_data(test) # Drop rows or columns with missing data\n    test = drop_nonunique_values(test)# Remove non-unique columns (e.g., constant features)\n    test = columns_transform(test)  # Apply any additional transformations (e.g., encoding)\n    \n    # Step 2: Apply PCA transformation if needed (Ensure 'pca' is already fitted)\n    test = pca.transform(test)  # This step assumes `pca` is a previously fitted PCA model\n    \n    # Step 3: Make predictions using the trained LightGBM model\n    predictions = lgbm_reg.predict(test)  # lgbm_reg should be your trained LightGBM model\n    \n    # Step 4: Add the predictions as a new column in the submission DataFrame\n    # Ensure `target` is the name of the target column\n    return predictions\npredict(test)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-11T14:13:38.703769Z","iopub.status.idle":"2024-09-11T14:13:38.704185Z","shell.execute_reply.started":"2024-09-11T14:13:38.703982Z","shell.execute_reply":"2024-09-11T14:13:38.704003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-09-11T14:13:38.705608Z","iopub.status.idle":"2024-09-11T14:13:38.706028Z","shell.execute_reply.started":"2024-09-11T14:13:38.705823Z","shell.execute_reply":"2024-09-11T14:13:38.705845Z"},"trusted":true},"execution_count":null,"outputs":[]}]}